{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61075be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… STEP 1: Import Libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# âœ… STEP 2: Upload JSON File\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# âœ… STEP 3: Load Dataset from JSON\n",
    "import io\n",
    "data = []\n",
    "for filename in uploaded.keys():\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n",
    "\n",
    "# âœ… STEP 4: Data Preprocessing\n",
    "sentences = df['headline'].values\n",
    "labels = df['is_sarcastic'].values\n",
    "\n",
    "# Tokenize text\n",
    "vocab_size = 10000\n",
    "max_length = 32\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… STEP 5: Build LSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=max_length),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# âœ… STEP 6: Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=128)\n",
    "\n",
    "# âœ… STEP 7: Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# âœ… STEP 8: Plot Accuracy and Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# âœ… STEP 9: Test on Custom Headline\n",
    "def predict_headline(headline):\n",
    "    seq = tokenizer.texts_to_sequences([headline])\n",
    "    padded_seq = pad_sequences(seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    prediction = model.predict(padded_seq)[0][0]\n",
    "    label = \"ðŸŸ¢ Genuine\" if prediction < 0.5 else \"ðŸ”´ Sarcastic / Fake\"\n",
    "    print(f\"Headline: {headline}\\nPrediction Score: {prediction:.2f} â†’ {label}\")\n",
    "\n",
    "# âž• Example\n",
    "predict_headline(\"Scientists discover water on Mars\")\n",
    "predict_headline(\"World ends tomorrow, women and minorities hardest hit\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
